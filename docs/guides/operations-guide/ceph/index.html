<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-guides/operations-guide/ceph" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.0.0">
<title data-rh="true">Ceph operations cheatsheet | OSISM</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://your-docusaurus-test-site.com/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://your-docusaurus-test-site.com/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://your-docusaurus-test-site.com/docs/guides/operations-guide/ceph"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Ceph operations cheatsheet | OSISM"><meta data-rh="true" name="description" content="Where to find docs"><meta data-rh="true" property="og:description" content="Where to find docs"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://your-docusaurus-test-site.com/docs/guides/operations-guide/ceph"><link data-rh="true" rel="alternate" href="https://your-docusaurus-test-site.com/docs/guides/operations-guide/ceph" hreflang="en"><link data-rh="true" rel="alternate" href="https://your-docusaurus-test-site.com/docs/guides/operations-guide/ceph" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="OSISM RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="OSISM Atom Feed"><link rel="stylesheet" href="/assets/css/styles.9392b5bd.css">
<script src="/assets/js/runtime~main.66967f80.js" defer="defer"></script>
<script src="/assets/js/main.24aef49b.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="OSISM Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/logo.svg" alt="OSISM Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">OSISM</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/intro/">Docs</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/osism/osism.github.io" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/intro/">Introduction</a><button aria-label="Expand sidebar category &#x27;Introduction&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/getting-started">Getting Started</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" aria-expanded="true" href="/docs/guides/">Guides</a><button aria-label="Collapse sidebar category &#x27;Guides&#x27;" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/docs/guides/deploy-guide/">Deploy Guide</a><button aria-label="Expand sidebar category &#x27;Deploy Guide&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/docs/guides/upgrade-guide/">Upgrade Guide</a><button aria-label="Expand sidebar category &#x27;Upgrade Guide&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/docs/guides/configuration-guide/">Configuration Guide</a><button aria-label="Expand sidebar category &#x27;Configuration Guide&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" aria-expanded="true" tabindex="0" href="/docs/guides/operations-guide/">Operations Guide</a><button aria-label="Collapse sidebar category &#x27;Operations Guide&#x27;" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/docs/guides/operations-guide/manager/">Manager</a><button aria-label="Expand sidebar category &#x27;Manager&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/guides/operations-guide/ceph">Ceph</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/docs/guides/operations-guide/openstack/">OpenStack</a><button aria-label="Expand sidebar category &#x27;OpenStack&#x27;" type="button" class="clean-btn menu__caret"></button></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/docs/guides/troubleshooting-guide/">Troubleshooting Guide</a><button aria-label="Expand sidebar category &#x27;Troubleshooting Guide&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/docs/guides/other-guides/">Other Guides</a><button aria-label="Expand sidebar category &#x27;Other Guides&#x27;" type="button" class="clean-btn menu__caret"></button></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/references/">References</a><button aria-label="Expand sidebar category &#x27;References&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/release-notes/">Release Notes</a><button aria-label="Expand sidebar category &#x27;Release Notes&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/appendix/">Appendix</a><button aria-label="Expand sidebar category &#x27;Appendix&#x27;" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/docs/guides/"><span itemprop="name">Guides</span></a><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/docs/guides/operations-guide/"><span itemprop="name">Operations Guide</span></a><meta itemprop="position" content="2"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Ceph</span><meta itemprop="position" content="3"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><h1 id="ceph-operations-cheatsheet">Ceph operations cheatsheet</h1>
<h2 id="where-to-find-docs">Where to find docs</h2>
<p>The official Ceph documentation is located on <a href="https://docs.ceph.com/en/latest">https://docs.ceph.com/en/latest</a></p>
<p>It is <strong>strongly advised</strong> to use the documentation for the version being used.</p>
<ul>
<li>Pacific - <a href="https://docs.ceph.com/en/pacific">https://docs.ceph.com/en/pacific</a></li>
<li>Quincy - <a href="https://docs.ceph.com/en/quincy">https://docs.ceph.com/en/quincy</a></li>
<li>Reef - <a href="https://docs.ceph.com/en/reef">https://docs.ceph.com/en/reef</a></li>
</ul>
<p>ceph-ansible documentation is located on <a href="https://docs.ceph.com/projects/ceph-ansible/en/latest/">https://docs.ceph.com/projects/ceph-ansible/en/latest/</a></p>
<admonition type="note"><p>Do not take information in the documentation at face value.
Especially when it comes to advanced/rarely used/very new features it is <strong>strongly advised</strong>
to test any claims made in the documentation about any particular feature.</p><p>Never assume that things will work as written without actually testing it on a test setup
as close to your real workload scenario as possible.</p></admonition>
<h2 id="advice-on-ceph-releases">Advice on Ceph releases</h2>
<p>The current Ceph releases and their support status can be found on <a href="https://docs.ceph.com/en/latest/releases/">https://docs.ceph.com/en/latest/releases/</a></p>
<p>When a new Ceph stable version is released you are <strong>strongly advised</strong>
to not roll it out on any production cluster whatsoever.
Even though its listed as &quot;stable&quot; it doesn&#x27;t mean that this is actually true.
Especially avoid using .0 releases on anything remotely production
unless you really, really now what you&#x27;re doing and can live with a possible catastrophic failure.</p>
<p>Be <strong>very</strong> conservative about what version you run on production systems.</p>
<p>Shiny new features aren&#x27;t worth the risk of total or partial data loss/corruption.</p>
<h2 id="general-maintenance">General maintenance</h2>
<h3 id="60-seconds-cluster-overview">60 seconds cluster overview</h3>
<p>The following commands can be used to quickly check the status of Ceph:</p>
<pre><code>$ ceph -s # Print overall cluster status
$ ceph health detail # Print detailed health information
$ ceph osd tree # Display current OSD tree
$ ceph df # Cluster storage usage by pool and storage class
$ ceph osd pool ls detail # List pools with detailed configuration
$ ceph osd df {plain|tree} {class e.g. hdd|ssd} # Get usage stats for OSDs
$ ceph -w # Watch Ceph health messages sequentially
$ ceph versions # List daemon versions running in the cluster
</code></pre>
<p>Also you can run the following on each node running ceph-daemons,
to provide further debug information about the environment:</p>
<pre><code># lscpu
# cat /proc/cpuinfo # if lscpu isn&#x27;t available
# free -g
# ip l
# ethtool &lt;device&gt; # for each network adapter
</code></pre>
<h3 id="muteunmute-a-health-warning">Mute/Unmute a health warning</h3>
<pre><code>$ ceph health mute &lt;what&gt; &lt;duration&gt;
$ ceph health unmute &lt;what&gt;
</code></pre>
<h3 id="disableenable-deep-scrubbing">Disable/Enable (deep-)scrubbing</h3>
<pre><code>$ ceph osd set noscrub
$ ceph osd set nodeep-scrub
$ ceph osd unset noscrub
$ ceph osd unset nodeep-scrub
</code></pre>
<admonition type="warning"><p>Use this sparingly only in emergency situations.
Setting these flags will cause a HEALTH_WARN status,
increase risk of data corruption and also the risk of generating
a HEALTH_WARN due to PGs not being (deep-)scrubbed in time.</p></admonition>
<h3 id="reboot-a-single-node">Reboot a single node</h3>
<p>The traditional way of doing this is by setting the <code>noout</code> flag,
do the appropriate maintenance work and after the node is back online
unset the flag like so:</p>
<pre><code>ceph osd set noout
</code></pre>
<p>After maintenance is done and host is back up:</p>
<pre><code>ceph osd unset noout
</code></pre>
<p>On versions Luminous or above you can set the flag individually for single
OSDs or entire CRUSH buckets, which can be a safer option in case of prolonged
maintenance periods.</p>
<p>Add noout for a OSD:</p>
<pre><code>ceph osd add-noout osd.&lt;ID&gt;
</code></pre>
<p>Remove noout for a OSD:</p>
<pre><code>ceph osd rm-noout osd.&lt;ID&gt;
</code></pre>
<p>Add noout for CRUSH bucket (e.g. host name as seen in <code>ceph osd tree</code>):</p>
<pre><code>ceph osd set-group noout &lt;crush-bucket-name&gt;
</code></pre>
<p>Remove noout for CRUSH bucket:</p>
<pre><code>ceph osd unset-group noout &lt;crush-bucket-name&gt;
</code></pre>
<h2 id="gathering-information-about-block-devices">Gathering information about block devices</h2>
<h3 id="enumerate-typical-storage-devices-and-lvm">Enumerate typical storage devices and LVM</h3>
<pre><code># lsblk
# lsblk -S
# lsscsi
# nvme list
# pvs
# vgs
# lvs
</code></pre>
<h3 id="smart-data-for-satasas-and-nvme-devices">SMART data for SATA/SAS and NVME devices</h3>
<pre><code># smartctl -a /dev/sdX
# nvme smart-log /dev/nvmeXnY
</code></pre>
<h3 id="check-format-of-a-nvme-device">Check format of a NVME device</h3>
<pre><code># nvme id-ns -H /dev/nvmeXnY
</code></pre>
<admonition type="note"><p>Check the last lines named &quot;LBA Format&quot;.
It will show which formats are supported,
which format is in use and which format offers the best performance
according to the vendor.</p></admonition>
<h3 id="format-a-nvme-device-to-a-different-lba-format-using-nvme-cli">Format a NVME device to a different LBA format using nvme-cli</h3>
<admonition type="warning"><p>This will destroy all data on the device!</p></admonition>
<pre><code># nvme format --lbaf=&lt;id&gt; /dev/nvmeXnY
</code></pre>
<h3 id="secure-erase-a-nvme-drive-using-nvme-cli">Secure Erase a NVME drive using nvme-cli</h3>
<admonition type="warning"><p>This will destroy all data on the device!</p></admonition>
<pre><code># nvme format -s2 /dev/nvmeXnY
# blkdiscard /dev/nvmeXnY
# nvme format -s1 /dev/nvmeXnY
</code></pre>
<h3 id="secure-erase-a-satasas-drive-using-hdparm">Secure Erase a SATA/SAS drive using hdparm</h3>
<admonition type="warning"><p>This will destroy all data on the device!</p></admonition>
<ol>
<li>
<p>Gather device info:</p>
<pre><code># hdparm -I /dev/sdX
</code></pre>
</li>
</ol>
<p>Check that the output says <strong>&quot;not frozen&quot;</strong> and <strong>&quot;not locked&quot;</strong>,
also it should list support for enhanced erase and list time estimates
for <strong>SECURITY ERASE UNIT</strong> and/or <strong>ENHANCED SECURITY ERASE UNIT</strong></p>
<ol start="2">
<li>
<p>Set a master password for the disk (required, will be automatically removed after wipe)</p>
<pre><code># hdparm --user-master wipeit --security-set-pass wipeit /dev/sdX
# hdparm -I /dev/sdX
</code></pre>
<p>Check that &quot;Security level&quot; is now <strong>&quot;high&quot;</strong> and master password is now
<strong>&quot;enabled&quot;</strong> instead of <strong>&quot;not enabled&quot;</strong> before</p>
</li>
<li>
<p>Wipe the device</p>
<p>If device supports enhanced security erase (better), use the following:</p>
<pre><code># hdparm --user-master wipeit --security-erase-enhanced wipeit /dev/sdX
</code></pre>
<p>If not, use standard security erase:</p>
<pre><code># hdparm --user-master wipeit --security-erase wipeit /dev/sdX
</code></pre>
</li>
</ol>
<admonition type="note"><p>On some systems the system firmware might &quot;freeze&quot; the device,
which makes it impossible to issue a secure erase or reformat the device.
In that case it might be necessary to either &quot;unfreeze&quot; the drive or
to install the drive in another system where it can be unfrozen.
Also make sure that the device is <em>actually</em> wiped. Its recommended to
at least perform a blanking pass on HDDs with a tool like nwipe.</p></admonition>
<h2 id="osd-maintenance-tasks">OSD maintenance tasks</h2>
<h3 id="locate-a-specific-osd-in-the-cluster">Locate a specific OSD in the cluster</h3>
<pre><code>$ ceph osd find osd.&lt;ID&gt;
</code></pre>
<h3 id="get-osd-metadata-global-and-single-osd">Get OSD metadata (global and single OSD)</h3>
<pre><code>$ ceph osd metadata
$ ceph osd metadata osd.&lt;ID&gt;
</code></pre>
<admonition type="note"><p>Interesting fields:
osd_objectstore, rotational, hostname, devices, device_ids, device_paths,
bluefs_db_rotational, bluefs_wal_rotational,
bluefs_dedicated_db, bluefs_dedicated_wal,
bluestore_bdev_rotational</p></admonition>
<h3 id="add-a-new-osd-using-ceph-ansible">Add a new OSD using ceph-ansible</h3>
<h3 id="remove-a-osd-using-ceph-ansible">Remove a OSD using ceph-ansible</h3>
<h3 id="replace-a-defect-osd">Replace a defect OSD</h3>
<h3 id="remove-a-single-osd-node">Remove a single OSD node</h3>
<h3 id="remove-an-osd-removing-it-completely-not-reprovisioning-it-again-without-double-rebalance">Remove an OSD (removing it completely, not reprovisioning it again) without double rebalance</h3>
<pre><code>$ ceph osd crush reweight osd.&lt;ID&gt; 0.0
... Wait for rebalance to complete, then mark it OUT:
$ ceph osd out osd.&lt;ID&gt;
# systemctl stop ceph-osd@&lt;ID&gt;
# systemctl disable ceph-osd@&lt;ID&gt;
$ ceph osd purge osd.&lt;ID&gt; --yes-i-really-mean-it
</code></pre>
<h3 id="remove-an-osd-temporarily-eg-when-replacing-a-broken-disk">Remove an OSD (temporarily e.g. when replacing a broken disk)</h3>
<pre><code>$ ceph osd out osd.&lt;ID&gt;
# systemctl stop ceph-osd@&lt;ID&gt;
# systemctl disable ceph-osd@&lt;ID&gt;
</code></pre>
<h3 id="disable-backfillsrecovery-completely">Disable backfills/recovery completely</h3>
<admonition type="warning"><p>Use only in emergency situations!</p></admonition>
<pre><code>$ ceph osd set nobackfill
$ ceph osd set norecovery
$ ceph osd set norebalance
</code></pre>
<p>Unset the flags with <code>ceph osd unset &lt;flag&gt;</code>.</p>
<h3 id="rebalance-osds">Rebalance OSDs</h3>
<h2 id="placement-group-maintenance">Placement Group maintenance</h2>
<h3 id="dump-placement-groups">Dump placement groups</h3>
<p>Usually only useful when parsing it, so here are two ways to get the data:</p>
<pre><code>$ ceph pg dump
$ ceph pg dump --format=json-pretty
</code></pre>
<h3 id="query-a-pg-about-its-status">Query a PG about its status</h3>
<pre><code>$ ceph pg &lt;pgid&gt; query
</code></pre>
<h3 id="start-deep-scrubbing-of-a-placement-group">Start (deep-)scrubbing of a placement group</h3>
<pre><code>$ ceph pg scrub &lt;pgid&gt;
$ ceph pg deep-scrub &lt;pgid&gt;
</code></pre>
<admonition type="note"><p>Instructing a PG to (deep-)scrub does not mean that it will do so immediately,
it can take some time for the scrub to start.</p></admonition>
<h3 id="health_warn---large-omap-objects-found">HEALTH_WARN - Large omap objects found...</h3>
<p>Finding PGs which have large OMAP objects:</p>
<pre><code># ceph pg dump --format=json | jq &#x27;.pg_map.pg_stats[] |
select(.stat_sum.num_large_omap_objects != 0) |
(.pgid, .stat_sum.num_large_omap_objects, .up, .acting)&#x27;
</code></pre>
<p>(Remove the line breaks between the single quotes or <code>jq</code> might act weird!)</p>
<p>This will dump all PG IDs with large OMAP objects and their up/acting OSDs.
You then can grep the logs of these OSDs for <strong>&quot;Large omap object&quot;</strong>
to find the actual objects causing the health warning.</p>
<p>Also the PG ID before the dot is equal to the pool ID it belongs to.</p>
<p>In case the logs have been rotated, instruct those OSDs to do a deep-scrub
and watch the logs for the message to appear.</p>
<p>From there you can investigate the issue further,
mostly it&#x27;ll be due to the index of a RGW bucket getting too big due to too many objects,
thus resharding that bucket&#x27;s index will be necessary.</p>
<h3 id="instruct-a-pg-to-repair-in-case-of-scrub-errors-inconsistent-pg">Instruct a PG to repair in case of scrub errors (inconsistent PG)</h3>
<pre><code>$ ceph pg repair &lt;pgid&gt;
</code></pre>
<admonition type="note"><p>Recovery might not start immediately and might take some time.
You can query the status of the recovery through <code>ceph pg &lt;pgid&gt; query</code>.
Be sure to read the Ceph manual about this topic <em>thoroughly</em>:</p><p><a href="https://docs.ceph.com/en/latest/rados/operations/pg-repair/">https://docs.ceph.com/en/latest/rados/operations/pg-repair/</a></p></admonition>
<h2 id="rados-pool-maintenance">RADOS Pool maintenance</h2>
<admonition type="note"><p>Read the RADOS pool operations documentation in detail before playing around with pools.
Especially when considering making changes to the CRUSH map.
Wrong decisions there can lead to data loss or other catastrophic failures.</p><p><a href="https://docs.ceph.com/en/latest/rados/operations/pools/">https://docs.ceph.com/en/latest/rados/operations/pools/</a></p></admonition>
<h3 id="get-pools-and-their-configuration">Get pools and their configuration</h3>
<pre><code>$ ceph osd pool ls detail
</code></pre>
<h3 id="dump-all-crush-rules">Dump all CRUSH rules</h3>
<pre><code>$ ceph osd crush rule dump
</code></pre>
<h3 id="get-autoscaler-status">Get autoscaler status</h3>
<pre><code>$ ceph osd pool autoscale-status
</code></pre>
<h3 id="create-a-replicated-pool">Create a replicated pool</h3>
<pre><code>$ ceph osd pool create &lt;pool_name&gt; &lt;pg_num&gt; &lt;pgp_num&gt; replicated [&lt;crush_rule_name&gt;]
</code></pre>
<h3 id="enabling-an-application-on-a-pool">Enabling an application on a pool</h3>
<p>Required, otherwise a health warning will be raised after some time.</p>
<pre><code>$ ceph osd pool application enable &lt;pool_name&gt; &lt;application_name&gt; # Syntax
$ ceph osd pool application enable cinder rbd # Example
</code></pre>
<p>Typical application names are: rbd, rgw, cephfs</p>
<h3 id="delete-a-pool">Delete a pool</h3>
<admonition type="warning"><p>This will delete all data in that pool. There is no undo/undelete.</p></admonition>
<pre><code>$ ceph osd pool delete &lt;pool_name&gt; &lt;pool_name&gt; --yes-i-really-really-mean-it
</code></pre>
<admonition type="note"><p>In order to be able to delete pools, it has to be enabled on the monitors
by setting the <code>mon_allow_pool_delete</code> flag to true. Default is false.</p><p>See: <a href="https://docs.ceph.com/en/latest/rados/configuration/mon-config-ref">https://docs.ceph.com/en/latest/rados/configuration/mon-config-ref</a></p></admonition>
<h3 id="set-number-of-pgs-for-a-pool">Set number of PGs for a pool</h3>
<pre><code>$ ceph osd pool set &lt;poolname&gt; pg_num &lt;num_pgs&gt;
</code></pre>
<admonition type="note"><p>Num PGs must be a power of two! Be careful about changing number of PGs.
Changing pg_num to a new value will gradually increase pgp_num on newer versions of Ceph.</p><p>In older versions one also has to set pgp_num manually, either in increments or in one big leap.</p></admonition>
<h3 id="create-crush-rules-for-different-storage-classes">Create CRUSH rules for different storage classes</h3>
<pre><code>$ ceph osd crush rule create-replicated replicated_hdd default host hdd
$ ceph osd crush rule create-replicated replicated_ssd default host ssd
$ ceph osd crush rule create-replicated replicated_nvme default host nvme
</code></pre>
<h3 id="change-crush-rule-for-a-pool-move-pool">Change CRUSH rule for a pool (&quot;move pool&quot;)</h3>
<pre><code>$ ceph osd pool set &lt;poolname&gt; crush_rule &lt;rule_name&gt;
</code></pre>
<p>This can be used to move a pool from e.g. HDD to SSD or NVME class
or anything else that the new CRUSH rule specifies.</p>
<h2 id="advanced-topics">Advanced topics</h2>
<h3 id="configure-and-create-lvm-devices-for-ceph-ansible-using-osism">Configure and create LVM devices for ceph-ansible using OSISM</h3>
<p>For more advanced OSD layout requirements leave out the <code>devices</code> key
and instead use <code>lvm_volumes</code>.</p>
<p>Details for this can be found on the official <a href="https://docs.ceph.com/projects/ceph-ansible/en/latest/osds/scenarios.html">ceph-ansible OSD Scenario</a> documentation.</p>
<p>In order to aid in creating the <code>lvm_volumes</code> config entries and provision the LVM devices for them,
OSISM has the two playbooks <code>configure-lvm-devices</code> and <code>create-lvm-devices</code> available.</p>
<p>Their usage is as follows:</p>
<ol>
<li>
<p>For each Ceph storage node edit the file <code>inventory/host_vars/&lt;nodename&gt;.yml</code>
add a configuration like the following to it:</p>
<pre><code>ceph_osd_db_wal_devices_buffer_space_percent: 10
# optional percentage of VGs to leave free,
# defaults to false
# Can be helpful for SSD performance of some older SSD models
# or to extend lifetime of SSDs in general

ceph_db_devices:
  nvme0n1:            # required, PV for a DB VG
                      # Will be prefixed by /dev/ and can also be specified
                      # like &quot;by-path/foo&quot; or other things under /dev/
    num_osds: 6       # required, number of OSDs that shall be
                      # maximum deployed to this device
    db_size: 30 GB    # optional, if not set, defaults to
                      # (VG size - buffer space (if enabled)) / num_osds
ceph_wal_devices:
  nvme1n1:            # See above, PV for a WAL VG
    num_osds: 6       # See above
    wal_size: 2 GB    # optional, if not set, defaults to 2 GiB

ceph_db_wal_devices:
nvme2n1:              # See above, PV for combined WAL+DB VG
  num_osds: 3         # See above
    db_size: 30 GB    # See above, except that it also considers
                      # total WAL size when calculating LV sizes
    wal_size: 2 GB    # See above

ceph_osd_devices:
  sda:                # Device name, will be prefixed by /dev/, see above conventions
                      # This would create a &quot;block only&quot; OSD without DB/WAL
                      # In reality, to ensure each device is uniquely identifiable,
                      # you should use WWN or EUI-64
                      # (in that case the entry here would be something like 
                      # disk/by-id/wwn-&lt;something&gt; or disk/by-id/nvme-eui.&lt;something&gt;)
  sdb:                # Create an OSD with dedicated DB
    db_pv: nvme0n1    # Must be one device configured in ceph_db_devices
                      # or ceph_db_wal_devices
  sdc:                # Create an OSD with dedicated WAL
    wal_pv: nvme1n1   # Must be one device configured in ceph_wal_devices
                      # or ceph_db_wal_devices
  sdb:                # Create an OSD with dedicated DB/WAL residing on different devices
    db_pv: nvme0n1    # See above
    wal_pv: nvme1n1   # See above
  sdc:                # Create an OSD with dedicated DB/WAL residing on the same VG/PV
    db_pv: nvme2n1    # Must be one device configured in ceph_db_wal_devices
    wal_pv: nvme2n1   # Must be the same device configured in ceph_db_wal_devices
</code></pre>
</li>
<li>
<p>Push the configuration to your configuration repository and after that do the following</p>
<pre><code>$ osism apply configuration
$ osism apply facts
</code></pre>
</li>
<li>
<p>After the configuration has been pulled and facts updated,
you can run the LVM configuration playbook:</p>
<pre><code>$ osism apply ceph-configure-lvm-volumes [-l INVENTORY HOST PATTERN]
</code></pre>
<p>This will generate a new configuration file for each node in <code>/tmp</code>
on the first manager node named <code>&lt;nodename&gt;-ceph-lvm-configuration.yml</code>.</p>
</li>
<li>
<p>Take the generated configuration file from <code>/tmp</code> and <strong>replace the previously generated configuration</strong> for each node.</p>
</li>
<li>
<p>Push the updated configuration <strong>again</strong> to your configuration repository and re-run:</p>
<pre><code>$ osism apply configuration
$ osism apply facts
</code></pre>
</li>
<li>
<p>Finally you can let OSISM create the LVM devices for you, because
<code>ceph-ansible</code> will not do that. To do that you simply run:</p>
<pre><code>$ osism apply ceph-create-lvm-devices [-l INVENTORY HOST PATTERN]
</code></pre>
</li>
<li>
<p>Deploy OSDs with ceph-ansible</p>
<p>When everything has finished and is ready to be deployed,
you can run:</p>
<pre><code>$ osism apply ceph-osds [-l INVENTORY HOST PATTERN]
</code></pre>
<p>This should then have <code>ceph-ansible</code> create new OSDs on the node.</p>
</li>
</ol>
<h3 id="validating-ceph-using-osism-playbooks">Validating Ceph using OSISM playbooks</h3>
<p>For Ceph, special playbooks were added to validate the deployment status of
the OSD, MON and MGR services. The commands for use are <code>osism validate ceph-osds</code>,
<code>osism validate ceph-mons</code>, and <code>osism validate ceph-mgrs</code>.</p>
<p>These playbooks will validate that the deployed Ceph environment matches
the configuration and is overall in a healthy state. The playbooks will
generate report files in JSON format on the first manager node in <code>/opt/reports/validator</code>.</p>
<h3 id="shutdown-a-ceph-cluster">Shutdown a Ceph cluster</h3>
<p>In order to fully shutdown a Ceph cluster safely, you first do the following steps:</p>
<admonition type="warning"><p>Take GOOD NOTES of the unit names and OSD IDs running on each node.
You will need them to restart the cluster later.</p></admonition>
<ol>
<li>
<p>Stop the workload that is using the cluster</p>
<p>This will vary depending on your environment and is not covered here.</p>
</li>
<li>
<p>Pause/Stop operations on the cluster by setting flags</p>
<pre><code>$ ceph osd set noout
$ ceph osd set nobackfill
$ ceph osd set norecover
$ ceph osd set norebalance
$ ceph osd set nodown
$ ceph osd set pause
</code></pre>
</li>
<li>
<p>Stop and disable the <code>radosgw</code> services on all nodes (on each rgw node) (if RGW is used)</p>
<p>Get the name of the unit (globs not supported for disable) and
make a note of the unit name for that node:</p>
<pre><code># systemctl | grep ceph-radosgw
</code></pre>
<p>Then disable and stop the unit:</p>
<pre><code># systemctl disable --now ceph-radosgw@&lt;name&gt;.service
</code></pre>
</li>
<li>
<p>Stop all CephFS file systems (if CephFS is used)</p>
<p>List all Ceph file systems</p>
<pre><code>$ ceph fs ls
</code></pre>
<p>For each CephFS do:</p>
<pre><code>$ ceph fs &lt;file system name&gt; down true
</code></pre>
</li>
<li>
<p>After that disable and stop all <code>ceph-mds</code> services on all nodes (do this on each node)</p>
<p>Get the name of the unit (globs not supported for disable) and
make a note of the unit name for that node:</p>
<pre><code># systemctl | grep ceph-mds
</code></pre>
<pre><code># systemctl disable --now ceph-mds@&lt;unit&gt;.service
</code></pre>
</li>
<li>
<p>Stop and disable the <code>ceph-mgr</code> services on all nodes (do this on each node)</p>
<p>Get the name of the unit (globs not supported for disable) and
make a note of the unit name for that node:</p>
<pre><code># systemctl | grep ceph-mgr
</code></pre>
<pre><code># systemctl disable --now ceph-mgr@&lt;unit&gt;.service
</code></pre>
</li>
<li>
<p>Stop and disable the <code>ceph-osd</code> services on all nodes (do this on each node)</p>
<p>Get the names of the units (globs not supported for disable) and
make a note of the unit names for that node (best to save it to a file):</p>
<pre><code># systemctl | grep ceph-osd
</code></pre>
<p>For each OSD unit execute:</p>
<pre><code># systemctl disable ceph-osd@&lt;osd-id&gt;.service
</code></pre>
<p>Stop all OSDs at once:</p>
<pre><code># systemctl stop ceph-osd\*.service
</code></pre>
</li>
<li>
<p>Finally stop the <code>ceph-mon</code> services on all nodes (do this on each node)</p>
<p>Get the name of the unit (globs not supported for disable) and
make a note of the unit name for that node:</p>
<pre><code># systemctl | grep ceph-mon
</code></pre>
<pre><code># systemctl disable --now ceph-mon@&lt;unit&gt;.service
</code></pre>
</li>
</ol>
<h3 id="restart-a-ceph-cluster-after-manual-shutdown">Restart a Ceph cluster after manual shutdown</h3>
<admonition type="warning"><p>You will need the notes taken during shutdown of the unit names.
It <strong>can</strong> be done without, but then it&#x27;ll be way more work finding out the names.</p></admonition>
<p>In order to restart a Ceph cluster after performing a manual shutdown like described
in the section above, you do the following:</p>
<ol>
<li>
<p>Enable &amp; start the <code>ceph-mon</code> services on all nodes (do this on each node)</p>
<pre><code># systemctl enable --now ceph-mon@&lt;unit-name&gt;.service
</code></pre>
</li>
<li>
<p>Enable &amp; start the <code>ceph-osd</code> services on all nodes (do this on each node)</p>
<p>For each Ceph OSD on that node do:</p>
<pre><code># systemctl enable --now ceph-osd@&lt;osd-id&gt;.service
</code></pre>
<p>Depending on the number of OSDs on that node it can take a while.</p>
</li>
<li>
<p>Enable &amp; start the <code>ceph-mgr</code> services on all nodes (do this on each node)</p>
<pre><code># systemctl enable --now ceph-mgr@&lt;unit-name&gt;.service
</code></pre>
</li>
<li>
<p>Check the status of your cluster and wait for all OSDs to come online</p>
<p>You can watch the status periodically by running:</p>
<pre><code>$ watch ceph -s
</code></pre>
<p>You should wait until all OSDs are up + in again, before removing flags.</p>
</li>
<li>
<p>Remove flags to unpause operations</p>
<pre><code>$ ceph osd unset pause
$ ceph osd unset nodown
$ ceph osd unset noout
$ ceph osd unset nobackfill
$ ceph osd unset norecover
$ ceph osd unset norebalance
</code></pre>
</li>
<li>
<p>Wait for cluster to resume operations</p>
<p>See step #4 of this SOP.
Now you wait until the cluster seems &quot;happy enough&quot; to accept clients.
(i.e. rebalancing finished etc.)
Maybe it will complain about MDS being down, but that&#x27;s normal for now.</p>
</li>
<li>
<p>Enable &amp; start the <code>ceph-mds</code> services on each node (if CephFS is used)</p>
<pre><code># systemctl enable --now ceph-mds@&lt;unit&gt;.service
</code></pre>
</li>
<li>
<p>Start CephFS file systems again</p>
<p>List all Ceph file systems</p>
<pre><code>$ ceph fs ls
</code></pre>
<p>For each CephFS do:</p>
<pre><code>$ ceph fs &lt;file system name&gt; down false
</code></pre>
</li>
<li>
<p>Enable &amp; start the <code>radosgw</code> services on each node (if RGW is used)</p>
<pre><code># systemctl enable --now ceph-radosgw@&lt;name&gt;.service
</code></pre>
</li>
</ol>
<h2 id="where-and-how-to-get-further-help">Where and how to get further help</h2>
<p>Join the <strong>#ceph</strong> IRC channel on <strong>irc.oftc.net</strong>, state the problem with as many details as possible
including information about what steps have already been taken to solve the problem
also provide information from the command output from the &quot;60 seconds cluster overview&quot; above
through a pastebin or a similar service. In order for people to be able
to help, details and some patience are important.</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/osism/osism.github.io/tree/main/docs/guides/operations-guide/ceph.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vwxv"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/guides/operations-guide/manager/task"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Task</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/guides/operations-guide/openstack/"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">OpenStack</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#where-to-find-docs" class="table-of-contents__link toc-highlight">Where to find docs</a></li><li><a href="#advice-on-ceph-releases" class="table-of-contents__link toc-highlight">Advice on Ceph releases</a></li><li><a href="#general-maintenance" class="table-of-contents__link toc-highlight">General maintenance</a><ul><li><a href="#60-seconds-cluster-overview" class="table-of-contents__link toc-highlight">60 seconds cluster overview</a></li><li><a href="#muteunmute-a-health-warning" class="table-of-contents__link toc-highlight">Mute/Unmute a health warning</a></li><li><a href="#disableenable-deep-scrubbing" class="table-of-contents__link toc-highlight">Disable/Enable (deep-)scrubbing</a></li><li><a href="#reboot-a-single-node" class="table-of-contents__link toc-highlight">Reboot a single node</a></li></ul></li><li><a href="#gathering-information-about-block-devices" class="table-of-contents__link toc-highlight">Gathering information about block devices</a><ul><li><a href="#enumerate-typical-storage-devices-and-lvm" class="table-of-contents__link toc-highlight">Enumerate typical storage devices and LVM</a></li><li><a href="#smart-data-for-satasas-and-nvme-devices" class="table-of-contents__link toc-highlight">SMART data for SATA/SAS and NVME devices</a></li><li><a href="#check-format-of-a-nvme-device" class="table-of-contents__link toc-highlight">Check format of a NVME device</a></li><li><a href="#format-a-nvme-device-to-a-different-lba-format-using-nvme-cli" class="table-of-contents__link toc-highlight">Format a NVME device to a different LBA format using nvme-cli</a></li><li><a href="#secure-erase-a-nvme-drive-using-nvme-cli" class="table-of-contents__link toc-highlight">Secure Erase a NVME drive using nvme-cli</a></li><li><a href="#secure-erase-a-satasas-drive-using-hdparm" class="table-of-contents__link toc-highlight">Secure Erase a SATA/SAS drive using hdparm</a></li></ul></li><li><a href="#osd-maintenance-tasks" class="table-of-contents__link toc-highlight">OSD maintenance tasks</a><ul><li><a href="#locate-a-specific-osd-in-the-cluster" class="table-of-contents__link toc-highlight">Locate a specific OSD in the cluster</a></li><li><a href="#get-osd-metadata-global-and-single-osd" class="table-of-contents__link toc-highlight">Get OSD metadata (global and single OSD)</a></li><li><a href="#add-a-new-osd-using-ceph-ansible" class="table-of-contents__link toc-highlight">Add a new OSD using ceph-ansible</a></li><li><a href="#remove-a-osd-using-ceph-ansible" class="table-of-contents__link toc-highlight">Remove a OSD using ceph-ansible</a></li><li><a href="#replace-a-defect-osd" class="table-of-contents__link toc-highlight">Replace a defect OSD</a></li><li><a href="#remove-a-single-osd-node" class="table-of-contents__link toc-highlight">Remove a single OSD node</a></li><li><a href="#remove-an-osd-removing-it-completely-not-reprovisioning-it-again-without-double-rebalance" class="table-of-contents__link toc-highlight">Remove an OSD (removing it completely, not reprovisioning it again) without double rebalance</a></li><li><a href="#remove-an-osd-temporarily-eg-when-replacing-a-broken-disk" class="table-of-contents__link toc-highlight">Remove an OSD (temporarily e.g. when replacing a broken disk)</a></li><li><a href="#disable-backfillsrecovery-completely" class="table-of-contents__link toc-highlight">Disable backfills/recovery completely</a></li><li><a href="#rebalance-osds" class="table-of-contents__link toc-highlight">Rebalance OSDs</a></li></ul></li><li><a href="#placement-group-maintenance" class="table-of-contents__link toc-highlight">Placement Group maintenance</a><ul><li><a href="#dump-placement-groups" class="table-of-contents__link toc-highlight">Dump placement groups</a></li><li><a href="#query-a-pg-about-its-status" class="table-of-contents__link toc-highlight">Query a PG about its status</a></li><li><a href="#start-deep-scrubbing-of-a-placement-group" class="table-of-contents__link toc-highlight">Start (deep-)scrubbing of a placement group</a></li><li><a href="#health_warn---large-omap-objects-found" class="table-of-contents__link toc-highlight">HEALTH_WARN - Large omap objects found...</a></li><li><a href="#instruct-a-pg-to-repair-in-case-of-scrub-errors-inconsistent-pg" class="table-of-contents__link toc-highlight">Instruct a PG to repair in case of scrub errors (inconsistent PG)</a></li></ul></li><li><a href="#rados-pool-maintenance" class="table-of-contents__link toc-highlight">RADOS Pool maintenance</a><ul><li><a href="#get-pools-and-their-configuration" class="table-of-contents__link toc-highlight">Get pools and their configuration</a></li><li><a href="#dump-all-crush-rules" class="table-of-contents__link toc-highlight">Dump all CRUSH rules</a></li><li><a href="#get-autoscaler-status" class="table-of-contents__link toc-highlight">Get autoscaler status</a></li><li><a href="#create-a-replicated-pool" class="table-of-contents__link toc-highlight">Create a replicated pool</a></li><li><a href="#enabling-an-application-on-a-pool" class="table-of-contents__link toc-highlight">Enabling an application on a pool</a></li><li><a href="#delete-a-pool" class="table-of-contents__link toc-highlight">Delete a pool</a></li><li><a href="#set-number-of-pgs-for-a-pool" class="table-of-contents__link toc-highlight">Set number of PGs for a pool</a></li><li><a href="#create-crush-rules-for-different-storage-classes" class="table-of-contents__link toc-highlight">Create CRUSH rules for different storage classes</a></li><li><a href="#change-crush-rule-for-a-pool-move-pool" class="table-of-contents__link toc-highlight">Change CRUSH rule for a pool (&quot;move pool&quot;)</a></li></ul></li><li><a href="#advanced-topics" class="table-of-contents__link toc-highlight">Advanced topics</a><ul><li><a href="#configure-and-create-lvm-devices-for-ceph-ansible-using-osism" class="table-of-contents__link toc-highlight">Configure and create LVM devices for ceph-ansible using OSISM</a></li><li><a href="#validating-ceph-using-osism-playbooks" class="table-of-contents__link toc-highlight">Validating Ceph using OSISM playbooks</a></li><li><a href="#shutdown-a-ceph-cluster" class="table-of-contents__link toc-highlight">Shutdown a Ceph cluster</a></li><li><a href="#restart-a-ceph-cluster-after-manual-shutdown" class="table-of-contents__link toc-highlight">Restart a Ceph cluster after manual shutdown</a></li></ul></li><li><a href="#where-and-how-to-get-further-help" class="table-of-contents__link toc-highlight">Where and how to get further help</a></li></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/osism" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2023 OSISM GmbH. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>