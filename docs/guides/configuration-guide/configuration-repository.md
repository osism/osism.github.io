---
sidebar_label: Configuration repository
sidebar_position: 10
---

# Configuration Repository

The configuration required for OSISM is stored in a single Git monorepo, the configuration repository.

## Configuration repository layout

A configuration repository is always composed of the same basic layout.

* **`environments` directory**
* **`inventory` directory**
* **`netbox` directory (optional)**
* **`requirements.txt` file**

  In the `requirements.txt` the necessary dependencies are listed to be able to execute Gilt.

* **`gilt.yml` file**

  [Gilt](https://gilt.readthedocs.io) is a Git layering tool. We use Gilt to maintain the image versions,
  Ansible configuration and scripts within the `environments/manager` directory.

  The [current gilt.yml](https://github.com/osism/cfg-generics/blob/main/gilt.yml) file is always located in the [osism/cfg-generics](https://github.com/osism/cfg-generics) repository.


## Creating a new configuration repository

The initial content for this repository is generated using the
[cookiecutter](https://github.com/osism/cfg-cookiecutter).

[Cookiecutter](https://github.com/cookiecutter/cookiecutter) generates a bootstrap configuration for your 
new cluster by prompting you for the basic details of the new system environment.

### Step 1: Preparation


#### Decide where to store the Git repository

The content generated by the cookiecutter in the `output/configuration` directory is
committed to a new Git repository. By default, it is assumed that the configuration
repository is stored on GitHub. This can also be GitLab or an internal Git service
as well.

Host and path to the Git repository are specified via the `git_` parameters: The
`git_` parameters do not specify the path to the cookiecutter to use.

```
  [8/20] git_host (github.com):
  [9/20] git_port (22):
  [10/20] git_repository (YOUR_ORG/YOUR_NEW_CONFIGURATION_REPOSITORY): regiocloud/configuration
  [11/20] git_username (git):
  [12/20] git_version (main):
```

In this case, the generated configuration in the `output/configuration` directory is
stored on GitHub in the `regiocloud/configuration` repository.


#### Have a look at the configuration reference

At the end of the chapter you find the configuration reference.

The parameters listed there are queried during the execution of Cookiecutter.


### Step 2: Execute cookiecutter to generate the basic layout/configuration

In this example a new configuration repository is created with the defaults. The current stable
version of OSISM is used. The use of latest is described in the section
[Use of latest](#use-of-latest).

The directory `output` is created and used as output volume.
```
mkdir output
```

The cookiecutter is executed within a container. Docker must be usable on the system
on which the cookiecutter is to be used. It should also work with podman.

```
docker run --net=host --rm -v $(pwd)/output:/output -it quay.io/osism/cookiecutter
[1/20] with_ceph (1):
[2/20] ceph_network_backend (192.168.80.0/20):
[3/20] ceph_network_frontend (192.168.64.0/20):
[4/20] ceph_version (quincy):
[5/20] domain (osism.xyz):
[6/20] fqdn_external (api.osism.xyz):
[7/20] fqdn_internal (api-int.osism.xyz):
[8/20] git_host (github.com):
[9/20] git_port (22):
[10/20] git_repository (YOUR_ORG/YOUR_NEW_CONFIGURATION_REPOSITORY):
[11/20] git_username (git):
[12/20] git_version (main):
[13/20] ip_external (192.168.96.9):
[14/20] ip_internal (192.168.32.9):
[15/20] manager_version (6.0.2):
[16/20] name_server (149.112.112.112):
[17/20] ntp_server (de.pool.ntp.org):
[18/20] openstack_version (2023.1):
[19/20] project_name (configuration):
[...]
```



### Step 3: Upload the new configuration to the remote git repository

Since we run the cookiecutter inside a container, the user rights are not correct
and have to be changed after the generation process.
```
sudo chown -R $USER output/
```

Add the initial configuration state to the repository
```
$ git clone git@github.com:YOUR_ORG/YOUR_NEW_CONFIGURATION_REPOSITORY.git YOUR_NEW_CONFIGURATION_REPOSITORY
$ cp -r output/COOKIECUTTER_PROJECT_NAME/{*,.gitignore} YOUR_NEW_CONFIGURATION_REPOSITORY
$ cd YOUR_NEW_CONFIGURATION_REPOSITORY
$ git add -A .
$ git commit -m "Initial commit after bootstrap"
$ git push
```

The content is now committed to the previously created Git repository.
The `secrets` directory:

* is not stored in the Git repository. Its contents should be stored in a trustworthy location
* contains an SSH key pair which is used as a deployment key to  make the configuration repository available 
  on the manager node later. Write access  is not required.
  (The public SSH key is stored in the file `secrets/id_rsa.configuration.pub`.)

How to add a deployment key on GitHub is documented in
[Managing deploy keys](https://docs.github.com/en/authentication/connecting-to-github-with-ssh/managing-deploy-keys).

### Step 4: Install the dependencies of the configuration repository

  To use Gilt the dependencies are installed first.

  Install all dependencies:
  ```
  cd YOUR_NEW_CONFIGURATION_REPOSITORY/YOUR_NEW_ENVIRONMENT
  make deps
  ```

### Step 5: Post-processing of the generated configuration

The configuration repository that is initially created with the Cookiecutter is not directly usable.

1. Important: Change the password for Ansible Vault encrypted files `secrets/vaultpass`.
   TODO: Add a make target for re-keying
2. Important: Change the password of the generated Keepass file i
   (Initial password is: `password`, Menu "File" â‡¢ "Change master key")
   ```commandline
   keepass2 secrets/keepass.kdbx
   ```
3. The creation or adaptation of the further configuration: Process all relevant 
   chapters beyond [Configuration Guide](./).

Version your configuration changes using git.

## Parameter Reference

<table>
  <tr>
    <th>Parameter</th>
    <th>Description</th>
    <th>Default</th>
  </tr>
  <tr>
    <td><code>ceph_network_backend</code></td>
    <td>Address range for ceph's backend network</td>
    <td><code>192.168.80.0/20</code></td>
  </tr>
  <tr>
    <td><code>ceph_network_frontend</code></td>
    <td>Address range for ceph's frontend network</td>
    <td><code>192.168.64.0/20</code></td>
  </tr>
  <tr>
    <td><code>ceph_version</code></td>
    <td>The version of Ceph. When using a stable OSISM release (<code>manager_version != latest</code>), this value is ignored.</td>
    <td><code>quincy</code></td>
  </tr>
  <tr>
    <td><code>domain</code></td>
    <td>The domain used by hostnames</td>
    <td><code>osism.xyz</code></td>
  </tr>
  <tr>
    <td><code>environment</code></td>
    <td>The name to identify the generated system landscape</td>
    <td><code>environment</code></td>
  </tr>
  <tr>
    <td><code>fqdn_external</code></td>
    <td>External API FQDN</td>
    <td><code>api.osism.xyz</code></td>
  </tr>
  <tr>
    <td><code>fqdn_internal</code></td>
    <td>Internal API FQDN</td>
    <td><code>api-int.osism.xyz</code></td>
  </tr>
  <tr>
    <td><code>git_host</code></td>
    <td>Address of the used Git server</td>
    <td><code>github.com</code></td>
  </tr>
  <tr>
    <td><code>git_port</code></td>
    <td>Port of the used Git server</td>
    <td><code>22</code></td>
  </tr>
  <tr>
    <td><code>git_repository</code></td>
    <td>Path to the git configuration repository</td>
    <td><code>YOUR_ORG/YOUR_NEW_CONFIGURATION_REPOSITORY</code></td>
  </tr>
  <tr>
    <td><code>git_username</code></td>
    <td>Username of the git repository</td>
    <td><code>git</code></td>
  </tr>
  <tr>
    <td><code>git_version</code></td>
    <td>Git branch name</td>
    <td><code>main</code></td>
  </tr>
  <tr>
    <td><code>ip_external</code></td>
    <td>The external IP address of the API (resolves to <code>fqdn_external</code>)</td>
    <td><code>192.168.96.9</code></td>
  </tr>
  <tr>
    <td><code>ip_internal</code></td>
    <td>The internal IP address of the API (resolves to <code>fqdn_internal</code>)</td>
    <td><code>192.168.32.9</code></td>
  </tr>
  <tr>
    <td><code>manager_version</code></td>
    <td>The version of OSISM. An overview of available OSISM releases can be found on <a href="https://release.osism.tech">release.osism.tech</a>.
      By default, this is always set to the current stable version.
      When you want to use latest this is done via the parameter `manager_version` (useful for development activities).
      If the `manager_version` parameter is set to `latest` it is also possible to explicitly
      set the `openstack_version` and the `ceph_version`.
</td>
    <td><code>6.0.2</code></td>
  </tr>
  <tr>
    <td><code>name_server</code></td>
    <td>Nameserver. Only one nameserver is set here because the query of multiple values in Cookiecutter is weird. Add more nameservers afterward.</td>
    <td><code>149.112.112.112</code></td>
  </tr>
  <tr>
    <td><code>ntp_server</code></td>
    <td>NTP server. Only one NTP server is set here because the query of multiple values in Cookiecutter is weird. Add more NTP servers afterward.</td>
    <td><code>de.pool.ntp.org</code></td>
  </tr>
  <tr>
    <td><code>openstack_version</code></td>
    <td>The version of OpenStack. When using a stable OSISM release (<code>manager_version != latest</code>), this value is ignored.</td>
    <td><code>2023.1</code></td>
  </tr>
  <tr>
    <td><code>project_name</code></td>
    <td>Name of the configuration repository directory</td>
    <td><code>configuration</code></td>
  </tr>
  <tr>
    <td><code>with_ceph</code></td>
    <td>1 to use Ceph, 0 to not use Ceph</td>
    <td><code>1</code></td>
  </tr>
</table>

## Preparing a new configuration repository

### Manager environment

```none title="environments/manager/hosts"
[manager]
manager01
```

```yaml title="environments/manager/host_vars/manager01.yml"
---
##########################################################
# ansible

ansible_host: 192.168.16.5

##########################################################
# generic

internal_interface: eno1

##########################################################
# network

network_type: netplan
network_ethernets:
  eno1:
    addresses:
      - "192.168.16.10/20"
    gateway4: "192.168.16.1"
    mtu: 1500
```

### Inventory

```none title="inventory/20-roles"
##########################################################
# roles

# NOTE: If netbox is not used, nothing needs to be changed here. In
#       this case this inventory is used as before. The hosts are
#       then managed here as normal.
#
#       If netbox is used this file is only used to store the hosts
#       for the initial import into the netbox.
#
#       After the initial import of the inventory in the netbox,
#       the groups in this file can be emptied. The systems are
#       then assigned to their roles via tags in the netbox.

# The "all" group is not used in OSISM. Therefore it is important
# that all nodes are explicitly listed here.
[generic]
node01

# Nodes that act as manager (sometimes called deployment node)
# are included in this group.
[manager]
manager01

# Nodes which are intended for monitoring services belong to
# this group
[monitoring]

# Nodes that serve as controllers, so things like scheduler,
# API or database run there, of the environment.
[control]

# Virtual systems managed by OpenStack Nova are placed on
# nodes in this group.
[compute]

# Network resources managed by OpenStack Neutron, such as
# L3 routers, are placed on these nodes. This group has nothing
# to do with the general network configuration.
[network]

# Nodes that serve as controllers for Ceph, so things like the
# Ceph Monitor service run here.
[ceph-control]

# The storage available in these systems is provided in the
# form of OSDs for Ceph.
[ceph-resource]

# NOTE: These empty groups are only necessary if netbox is used. After
#       the initial import of the hosts these groups can be commented
#       out. The groups above with the initial hosts can be commented.
#
# [generic]
#
# [manager]
#
# [monitoring]
#
# [control]
#
# [compute]
#
# [network]
#
# [ceph-control]
#
# [ceph-resource]
```

```yaml title="inventory/host_vars/node01.yml"
---
##########################################################
# ansible

# NOTE: Address where the node can be reached via SSH.
ansible_host: 192.168.16.10

##########################################################
# generic

internal_interface: eno1

# NOTE: The address of the internal interface.
internal_address: 192.168.16.10

##########################################################
# netdata

netdata_host_type: client

# NOTE: Uncomment this when this node should be a Netdata server.

# netdata_host_type: server

##########################################################
# network

# NOTE: This is the initial management interface. Further interfaces
#       must be added.

network_type: netplan
network_ethernets:
  eno1:
    addresses:
      - "192.168.16.10/20"
    gateway4: "192.168.16.1"
    mtu: 1500

##########################################################
# kolla

network_interface: eno1

# api_interface:
# bifrost_network_interface:
# dns_interface:
# kolla_external_vip_interface:
# migration_interface:
# neutron_external_interface:
# octavia_network_interface:
# storage_interface:
# tunnel_interface:

##########################################################
# ceph

# NOTE: Uncomment this when this node is a part of the Ceph cluster.

# monitor_address:
# radosgw_address:

# monitor_interface:
# radosgw_interface:

# NOTE: Uncomment this when this node should be a OSD node.

# devices:
#   - /dev/sdb
#   - /dev/sdc
#   - /dev/sdd
#   - /dev/sde
```
