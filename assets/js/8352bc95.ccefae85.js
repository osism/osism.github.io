"use strict";(self.webpackChunkosism=self.webpackChunkosism||[]).push([[3980],{2417:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>a,contentTitle:()=>o,default:()=>h,frontMatter:()=>t,metadata:()=>c,toc:()=>d});var s=i(5893),r=i(1151);const t={sidebar_label:"Ceph",sidebar_position:40},o="Ceph",c={id:"guides/configuration-guide/ceph",title:"Ceph",description:"Unique Identifier",source:"@site/docs/guides/configuration-guide/ceph.md",sourceDirName:"guides/configuration-guide",slug:"/guides/configuration-guide/ceph",permalink:"/docs/guides/configuration-guide/ceph",draft:!1,unlisted:!1,editUrl:"https://github.com/osism/osism.github.io/tree/main/docs/guides/configuration-guide/ceph.md",tags:[],version:"current",sidebarPosition:40,frontMatter:{sidebar_label:"Ceph",sidebar_position:40},sidebar:"tutorialSidebar",previous:{title:"Services",permalink:"/docs/guides/configuration-guide/services/"},next:{title:"OpenStack",permalink:"/docs/guides/configuration-guide/openstack/"}},a={},d=[{value:"Unique Identifier",id:"unique-identifier",level:2},{value:"Client",id:"client",level:2},{value:"Swappiness",id:"swappiness",level:2},{value:"RGW service",id:"rgw-service",level:2},{value:"Extra pools",id:"extra-pools",level:2},{value:"OSD Devices",id:"osd-devices",level:2}];function l(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",li:"li",ol:"ol",p:"p",pre:"pre",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,r.a)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h1,{id:"ceph",children:"Ceph"}),"\n",(0,s.jsx)(n.h2,{id:"unique-identifier",children:"Unique Identifier"}),"\n",(0,s.jsxs)(n.p,{children:["The File System ID is a unique identifier for the cluster.\nThe identifier is set via the parameter ",(0,s.jsx)(n.code,{children:"fsid"})," in ",(0,s.jsx)(n.code,{children:"environments/ceph/configuration.yml"}),"\nand must be unique. It can be generated with ",(0,s.jsx)(n.code,{children:"uuidgen"}),"."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",metastring:'title="environments/ceph/configuration.yml"',children:"fsid: c2120a4a-669c-4769-a32c-b7e9d7b848f4\n"})}),"\n",(0,s.jsx)(n.h2,{id:"client",children:"Client"}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"client.admin"})," keyring is placed in the file ",(0,s.jsx)(n.code,{children:"environments/infrastructure/files/ceph/ceph.client.admin.keyring"}),"."]}),"\n",(0,s.jsx)(n.h2,{id:"swappiness",children:"Swappiness"}),"\n",(0,s.jsxs)(n.p,{children:["The swappiness is set via the ",(0,s.jsx)(n.code,{children:"os_tuning_params"})," dictionary. The dictionary can\nonly be completely overwritten via an entry in the file ",(0,s.jsx)(n.code,{children:"environments/ceph/configuration.yml"}),"."]}),"\n",(0,s.jsx)(n.p,{children:"By default, the dictionary looks like this:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:'os_tuning_params:\n  - { name: fs.file-max, value: 26234859 }\n  - { name: vm.zone_reclaim_mode, value: 0 }\n  - { name: vm.swappiness, value: 10 }\n  - { name: vm.min_free_kbytes, value: "{{ vm_min_free_kbytes }}" }\n'})}),"\n",(0,s.jsxs)(n.p,{children:["The sysctl paremeters are written to the file ",(0,s.jsx)(n.code,{children:"/etc/sysctl.d/ceph-tuning.conf"}),"\non the storage nodes."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"# cat /etc/sysctl.d/ceph-tuning.conf\nfs.aio-max-nr=1048576\nfs.file-max=26234859\nvm.zone_reclaim_mode=0\nvm.swappiness=10\nvm.min_free_kbytes=4194303\n"})}),"\n",(0,s.jsx)(n.h2,{id:"rgw-service",children:"RGW service"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Add following configuration in ",(0,s.jsx)(n.code,{children:"environments/ceph/configuration.yml"})]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'ceph_conf_overrides:\n  "client.rgw.{{ hostvars[inventory_hostname][\'ansible_hostname\'] }}.rgw0":\n    "rgw content length compat": "true"\n    "rgw enable apis": "swift, s3, admin"\n    "rgw keystone accepted roles": "member, admin"\n    "rgw keystone accepted admin roles": "admin"\n    "rgw keystone admin domain": "default"\n    "rgw keystone admin password": "{{ ceph_rgw_keystone_password }}"\n    "rgw keystone admin project": "service"\n    "rgw keystone admin tenant": "service"\n    "rgw keystone admin user": "ceph_rgw"\n    "rgw keystone api version": "3"\n    "rgw keystone url": "https://api-int.testbed.osism.xyz:5000"\n    "rgw keystone verify ssl": "false"\n    "rgw keystone implicit tenants": "true"\n    "rgw s3 auth use keystone": "true"\n    "rgw swift account in url": "true"\n    "rgw swift versioning enabled": "true"\n'})}),"\n",(0,s.jsxs)(n.p,{children:["If the ",(0,s.jsx)(n.code,{children:"ceph_conf_overrides"})," parameter already exists in ",(0,s.jsx)(n.code,{children:"environments/ceph/configuration.yml"}),",\nexpand it and do not overwrite it."]}),"\n",(0,s.jsx)(n.p,{children:"If self-signed SSL certificates are used, two additional parameters must be set."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:' "rgw keystone verify ssl": "false"\n "rgw verify ssl": "false"\n'})}),"\n",(0,s.jsxs)(n.p,{children:["For all possible configuration parameters visit the\n",(0,s.jsx)(n.a,{href:"https://docs.ceph.com/en/quincy/radosgw/config-ref/",children:"Ceph configuration reference"}),"."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Add the ",(0,s.jsx)(n.code,{children:"ceph_rgw_keystone_password"})," from ",(0,s.jsx)(n.code,{children:"environments/kolla/secrets.yml"})," to\n",(0,s.jsx)(n.code,{children:"environments/ceph/secrets.yml"}),"."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Add following configuration in ",(0,s.jsx)(n.code,{children:"environments/kolla/configuration.yml"})]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:"enable_ceph_rgw: true\nenable_ceph_rgw_keystone: true\n\nceph_rgw_swift_compatibility: false\nceph_rgw_swift_account_in_url: true\n"})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"extra-pools",children:"Extra pools"}),"\n",(0,s.jsxs)(n.p,{children:["Extra pools can be defined via the ",(0,s.jsx)(n.code,{children:"openstack_pools_extra"})," parameter."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",metastring:'title="inventory/group_vars/generic/ceph.yml"',children:'openstack_cinder_extra001_pool:\n  name: extra001\n  pg_num: "{{ openstack_pool_default_pg_num }}"\n  pgp_num: "{{ openstack_pool_default_pg_num }}"\n  rule_name: "replicated_rule"\n  min_size: "{{ openstack_pool_default_min_size }}"\n  application: "rbd"\n\nopenstack_pools_extra:\n  - "{{ openstack_cinder_extra001_pool }}"\n'})}),"\n",(0,s.jsxs)(n.p,{children:["If more than one Ceph cluster is managed with one manager, do not place the\nparameters in ",(0,s.jsx)(n.code,{children:"inventory/group_vars/generic"})," but in a corresponding directory."]}),"\n",(0,s.jsxs)(n.p,{children:["If, for example, the inventory group of the Ceph cluster on which the additional\npools are to be created is ",(0,s.jsx)(n.code,{children:"ceph.rbd"}),", then the parameters would be stored in\n",(0,s.jsx)(n.code,{children:"inventory/group_vars/ceph.rbd.yml"})," accordingly."]}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Parameter"}),(0,s.jsx)(n.th,{children:"Default value"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"openstack_pool_default_pg_num"}),(0,s.jsx)(n.td,{children:"64"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"openstack_pool_default_min_size"}),(0,s.jsx)(n.td,{children:"0"})]})]})]}),"\n",(0,s.jsx)(n.h2,{id:"osd-devices",children:"OSD Devices"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:"ceph_osd_db_wal_devices_buffer_space_percent: 10\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'ceph_db_devices:\n  nvme0n1:         # required, PV for a DB VG\n                   # Will be prefixed by /dev/ and can also be specified\n                   # like "by-path/foo" or other things under /dev/\n    num_osds: 6    # required, number of OSDs that shall be\n                   # maximum deployed to this device\n    db_size: 30 GB # optional, if not set, defaults to\n                   # (VG size - buffer space (if enabled)) / num_osds\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:"ceph_wal_devices:\n  nvme1n1:         # See above, PV for a WAL VG\n    num_osds: 6    # See above\n    wal_size: 2 GB # optional, if not set, defaults to 2 GiB\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:"ceph_db_wal_devices:\n  nvme2n1:         # See above, PV for combined WAL+DB VG\n    num_osds: 3    # See above\n    db_size: 30 GB # See above, except that it also considers\n                   # total WAL size when calculating LV sizes\n    wal_size: 2 GB # See above\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'ceph_osd_devices:\n  sda:              # Device name, will be prefixed by /dev/, see above conventions\n                    # This would create a "block only" OSD without DB/WAL\n  sdb:              # Create an OSD with dedicated DB\n    db_pv: nvme0n1  # Must be one device configured in ceph_db_devices\n                    # or ceph_db_wal_devices\n  sdc:              # Create an OSD with dedicated WAL\n    wal_pv: nvme1n1 # Must be one device configured in ceph_wal_devices\n                    # or ceph_db_wal_devices\n  sdb:              # Create an OSD with dedicated DB/WAL residing on different devices\n    db_pv: nvme0n1  # See above\n    wal_pv: nvme1n1 # See above\n  sdc:              # Create an OSD with dedicated DB/WAL residing on the same VG/PV\n    db_pv: nvme2n1  # Must be one device configured in ceph_db_wal_devices\n    wal_pv: nvme2n1 # Must be the same device configured in ceph_db_wal_devices\n'})}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["Provide config stanza like above either in ",(0,s.jsx)(n.code,{children:"group_vars"})," or ",(0,s.jsx)(n.code,{children:"host_vars"}),"\nin the inventory of the configuration repository"]}),"\n",(0,s.jsxs)(n.li,{children:["Do ",(0,s.jsx)(n.code,{children:"osism reconciler sync"})," and ",(0,s.jsx)(n.code,{children:"osism apply facts"})]}),"\n",(0,s.jsxs)(n.li,{children:["Run the configuration playbook for the hosts you wish to configure:\n",(0,s.jsx)(n.code,{children:"osism apply ceph-configure-lvm-volumes -e ireallymeanit=yes"})]}),"\n",(0,s.jsxs)(n.li,{children:["The configuration generated for the hosts can be found on the\nmanager node of your setup in\n",(0,s.jsx)(n.code,{children:"/tmp/<inventory_hostname>-ceph-lvm-configuration.yml"})]}),"\n",(0,s.jsxs)(n.li,{children:["Add this configuration to your ",(0,s.jsx)(n.code,{children:"host_vars"})," for the nodes (see step 2)"]}),"\n",(0,s.jsxs)(n.li,{children:["Notice that the old config stanza has been expanded with UUIDs,\nif you use group_vars for config stanza, you should leave the group_vars\nuntouched and integrate the entire generated configuration into ",(0,s.jsx)(n.code,{children:"host_vars"}),"\nfor the nodes, as UUIDs are generated ",(0,s.jsx)(n.em,{children:"for each host"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:["After making sure that configuration is okay and synced and applied,\nyou can run the ",(0,s.jsx)(n.code,{children:"ceph-create-lvm-devices"})," playbook:\n",(0,s.jsx)(n.code,{children:"osism apply ceph-create-lvm-devices -e ireallymeanit=yes"})]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,r.a)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(l,{...e})}):l(e)}},1151:(e,n,i)=>{i.d(n,{Z:()=>c,a:()=>o});var s=i(7294);const r={},t=s.createContext(r);function o(e){const n=s.useContext(t);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),s.createElement(t.Provider,{value:n},e.children)}}}]);