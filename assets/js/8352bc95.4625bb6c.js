"use strict";(self.webpackChunkosism=self.webpackChunkosism||[]).push([[3980],{2417:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>d,contentTitle:()=>t,default:()=>h,frontMatter:()=>o,metadata:()=>c,toc:()=>a});var s=i(5893),r=i(1151);const o={sidebar_label:"Ceph",sidebar_position:40},t="Ceph",c={id:"guides/configuration-guide/ceph",title:"Ceph",description:"Unique Identifier",source:"@site/docs/guides/configuration-guide/ceph.md",sourceDirName:"guides/configuration-guide",slug:"/guides/configuration-guide/ceph",permalink:"/docs/guides/configuration-guide/ceph",draft:!1,unlisted:!1,editUrl:"https://github.com/osism/osism.github.io/tree/main/docs/guides/configuration-guide/ceph.md",tags:[],version:"current",sidebarPosition:40,frontMatter:{sidebar_label:"Ceph",sidebar_position:40},sidebar:"tutorialSidebar",previous:{title:"Services",permalink:"/docs/guides/configuration-guide/services/"},next:{title:"OpenStack",permalink:"/docs/guides/configuration-guide/openstack/"}},d={},a=[{value:"Unique Identifier",id:"unique-identifier",level:2},{value:"Client",id:"client",level:2},{value:"Swappiness",id:"swappiness",level:2},{value:"RGW service",id:"rgw-service",level:2},{value:"Extra pools",id:"extra-pools",level:2},{value:"LVM devices",id:"lvm-devices",level:2}];function l(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,r.a)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h1,{id:"ceph",children:"Ceph"}),"\n",(0,s.jsx)(n.h2,{id:"unique-identifier",children:"Unique Identifier"}),"\n",(0,s.jsxs)(n.p,{children:["The File System ID is a unique identifier for the cluster.\nThe identifier is set via the parameter ",(0,s.jsx)(n.code,{children:"fsid"})," in ",(0,s.jsx)(n.code,{children:"environments/ceph/configuration.yml"}),"\nand must be unique. It can be generated with ",(0,s.jsx)(n.code,{children:"uuidgen"}),"."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",metastring:'title="environments/ceph/configuration.yml"',children:"fsid: c2120a4a-669c-4769-a32c-b7e9d7b848f4\n"})}),"\n",(0,s.jsx)(n.h2,{id:"client",children:"Client"}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"client.admin"})," keyring is placed in the file ",(0,s.jsx)(n.code,{children:"environments/infrastructure/files/ceph/ceph.client.admin.keyring"}),"."]}),"\n",(0,s.jsx)(n.h2,{id:"swappiness",children:"Swappiness"}),"\n",(0,s.jsxs)(n.p,{children:["The swappiness is set via the ",(0,s.jsx)(n.code,{children:"os_tuning_params"})," dictionary. The dictionary can\nonly be completely overwritten via an entry in the file ",(0,s.jsx)(n.code,{children:"environments/ceph/configuration.yml"}),"."]}),"\n",(0,s.jsx)(n.p,{children:"By default, the dictionary looks like this:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:'os_tuning_params:\n  - { name: fs.file-max, value: 26234859 }\n  - { name: vm.zone_reclaim_mode, value: 0 }\n  - { name: vm.swappiness, value: 10 }\n  - { name: vm.min_free_kbytes, value: "{{ vm_min_free_kbytes }}" }\n'})}),"\n",(0,s.jsxs)(n.p,{children:["The sysctl paremeters are written to the file ",(0,s.jsx)(n.code,{children:"/etc/sysctl.d/ceph-tuning.conf"}),"\non the storage nodes."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"# cat /etc/sysctl.d/ceph-tuning.conf\nfs.aio-max-nr=1048576\nfs.file-max=26234859\nvm.zone_reclaim_mode=0\nvm.swappiness=10\nvm.min_free_kbytes=4194303\n"})}),"\n",(0,s.jsx)(n.h2,{id:"rgw-service",children:"RGW service"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Add following configuration in ",(0,s.jsx)(n.code,{children:"environments/ceph/configuration.yml"})]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'ceph_conf_overrides:\n  "client.rgw.{{ hostvars[inventory_hostname][\'ansible_hostname\'] }}.rgw0":\n    "rgw content length compat": "true"\n    "rgw enable apis": "swift, s3, admin"\n    "rgw keystone accepted roles": "member, admin"\n    "rgw keystone accepted admin roles": "admin"\n    "rgw keystone admin domain": "default"\n    "rgw keystone admin password": "{{ ceph_rgw_keystone_password }}"\n    "rgw keystone admin project": "service"\n    "rgw keystone admin tenant": "service"\n    "rgw keystone admin user": "ceph_rgw"\n    "rgw keystone api version": "3"\n    "rgw keystone url": "https://api-int.testbed.osism.xyz:5000"\n    "rgw keystone verify ssl": "false"\n    "rgw keystone implicit tenants": "true"\n    "rgw s3 auth use keystone": "true"\n    "rgw swift account in url": "true"\n    "rgw swift versioning enabled": "true"\n'})}),"\n",(0,s.jsxs)(n.p,{children:["If the ",(0,s.jsx)(n.code,{children:"ceph_conf_overrides"})," parameter already exists in ",(0,s.jsx)(n.code,{children:"environments/ceph/configuration.yml"}),",\nexpand it and do not overwrite it."]}),"\n",(0,s.jsx)(n.p,{children:"If self-signed SSL certificates are used, two additional parameters must be set."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:' "rgw keystone verify ssl": "false"\n "rgw verify ssl": "false"\n'})}),"\n",(0,s.jsxs)(n.p,{children:["For all possible configuration parameters visit the\n",(0,s.jsx)(n.a,{href:"https://docs.ceph.com/en/quincy/radosgw/config-ref/",children:"Ceph configuration reference"}),"."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Add the ",(0,s.jsx)(n.code,{children:"ceph_rgw_keystone_password"})," from ",(0,s.jsx)(n.code,{children:"environments/kolla/secrets.yml"})," to\n",(0,s.jsx)(n.code,{children:"environments/ceph/secrets.yml"}),"."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Add following configuration in ",(0,s.jsx)(n.code,{children:"environments/kolla/configuration.yml"})]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:"enable_ceph_rgw: true\nenable_ceph_rgw_keystone: true\n\nceph_rgw_swift_compatibility: false\nceph_rgw_swift_account_in_url: true\n"})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"extra-pools",children:"Extra pools"}),"\n",(0,s.jsxs)(n.p,{children:["Extra pools can be defined via the ",(0,s.jsx)(n.code,{children:"openstack_pools_extra"})," parameter."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",metastring:'title="inventory/group_vars/generic/ceph.yml"',children:'openstack_cinder_extra001_pool:\n  name: extra001\n  pg_num: "{{ openstack_pool_default_pg_num }}"\n  pgp_num: "{{ openstack_pool_default_pg_num }}"\n  rule_name: "replicated_rule"\n  min_size: "{{ openstack_pool_default_min_size }}"\n  application: "rbd"\n\nopenstack_pools_extra:\n  - "{{ openstack_cinder_extra001_pool }}"\n'})}),"\n",(0,s.jsxs)(n.p,{children:["If more than one Ceph cluster is managed with one manager, do not place the\nparameters in ",(0,s.jsx)(n.code,{children:"inventory/group_vars/generic"})," but in a corresponding directory."]}),"\n",(0,s.jsxs)(n.p,{children:["If, for example, the inventory group of the Ceph cluster on which the additional\npools are to be created is ",(0,s.jsx)(n.code,{children:"ceph.rbd"}),", then the parameters would be stored in\n",(0,s.jsx)(n.code,{children:"inventory/group_vars/ceph.rbd.yml"})," accordingly."]}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Parameter"}),(0,s.jsx)(n.th,{children:"Default value"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"openstack_pool_default_pg_num"})}),(0,s.jsx)(n.td,{children:"64"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"openstack_pool_default_min_size"})}),(0,s.jsx)(n.td,{children:"0"})]})]})]}),"\n",(0,s.jsx)(n.h2,{id:"lvm-devices",children:"LVM devices"}),"\n",(0,s.jsxs)(n.p,{children:["For more advanced OSD layout requirements leave out the ",(0,s.jsx)(n.code,{children:"devices"})," key\nand instead use ",(0,s.jsx)(n.code,{children:"lvm_volumes"}),". Details for this can be found on the\n",(0,s.jsx)(n.a,{href:"https://docs.ceph.com/projects/ceph-ansible/en/latest/osds/scenarios.html",children:"OSD Scenario"})," documentation."]}),"\n",(0,s.jsxs)(n.p,{children:["In order to aid in creating the ",(0,s.jsx)(n.code,{children:"lvm_volumes"})," config entries and provision the LVM devices for them,\nOSISM has the two playbooks ",(0,s.jsx)(n.code,{children:"ceph-configure-lvm-devices"})," and ",(0,s.jsx)(n.code,{children:"ceph-create-lvm-devices"})," available."]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["For each Ceph storage node edit the file ",(0,s.jsx)(n.code,{children:"inventory/host_vars/<nodename>.yml"}),"\nadd a configuration like the following to it:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:'# optional percentage of VGs to leave free,\n# defaults to false\n# Can be helpful for SSD performance of some older SSD models\n# or to extend lifetime of SSDs in general\n\nceph_osd_db_wal_devices_buffer_space_percent: 10\n\nceph_db_devices:\n  nvme0n1:            # required, PV for a DB VG\n                      # Will be prefixed by /dev/ and can also be specified\n                      # like "by-path/foo" or other things under /dev/\n    num_osds: 6       # required, number of OSDs that shall be\n                      # maximum deployed to this device\n    db_size: 30 GB    # optional, if not set, defaults to\n                      # (VG size - buffer space (if enabled)) / num_osds\nceph_wal_devices:\n  nvme1n1:            # See above, PV for a WAL VG\n    num_osds: 6       # See above\n    wal_size: 2 GB    # optional, if not set, defaults to 2 GiB\n\nceph_db_wal_devices:\nnvme2n1:              # See above, PV for combined WAL+DB VG\n  num_osds: 3         # See above\n    db_size: 30 GB    # See above, except that it also considers\n                      # total WAL size when calculating LV sizes\n    wal_size: 2 GB    # See above\n\nceph_osd_devices:\n  sda:                # Device name, will be prefixed by /dev/, see above conventions\n                      # This would create a "block only" OSD without DB/WAL\n                      # In reality, to ensure each device is uniquely identifiable,\n                      # you should use WWN or EUI-64\n                      # (in that case the entry here would be something like \n                      # disk/by-id/wwn-<something> or disk/by-id/nvme-eui.<something>)\n  sdb:                # Create an OSD with dedicated DB\n    db_pv: nvme0n1    # Must be one device configured in ceph_db_devices\n                      # or ceph_db_wal_devices\n  sdc:                # Create an OSD with dedicated WAL\n    wal_pv: nvme1n1   # Must be one device configured in ceph_wal_devices\n                      # or ceph_db_wal_devices\n  sdb:                # Create an OSD with dedicated DB/WAL residing on different devices\n    db_pv: nvme0n1    # See above\n    wal_pv: nvme1n1   # See above\n  sdc:                # Create an OSD with dedicated DB/WAL residing on the same VG/PV\n    db_pv: nvme2n1    # Must be one device configured in ceph_db_wal_devices\n    wal_pv: nvme2n1   # Must be the same device configured in ceph_db_wal_devices\n'})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Push the configuration to your configuration repository and after that do the following"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"$ osism apply configuration\n$ osism reconciler sync\n$ osism apply facts\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"After the configuration has been pulled and facts updated,\nyou can run the LVM configuration playbook:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"$ osism apply ceph-configure-lvm-volumes\n"})}),"\n",(0,s.jsxs)(n.p,{children:["This will generate a new configuration file for each node in ",(0,s.jsx)(n.code,{children:"/tmp"}),"\non the first manager node named ",(0,s.jsx)(n.code,{children:"<nodename>-ceph-lvm-configuration.yml"}),"."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Take the generated configuration file from ",(0,s.jsx)(n.code,{children:"/tmp"})," and ",(0,s.jsx)(n.strong,{children:"replace the previously\ngenerated configuration"})," for each node."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Push the updated configuration ",(0,s.jsx)(n.strong,{children:"again"})," to your configuration repository and re-run:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"$ osism apply configuration\n$ osism reconciler sync\n$ osism apply facts\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Finally let OSISM create the LVM devices for you."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"$ osism apply ceph-create-lvm-devices\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Deploy OSDs with ",(0,s.jsx)(n.code,{children:"ceph-osds"}),"."]}),"\n",(0,s.jsx)(n.p,{children:"When everything has finished and is ready to be deployed,\nyou can run:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"$ osism apply ceph-osds\n"})}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,r.a)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(l,{...e})}):l(e)}},1151:(e,n,i)=>{i.d(n,{Z:()=>c,a:()=>t});var s=i(7294);const r={},o=s.createContext(r);function t(e){const n=s.useContext(o);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:t(e.components),s.createElement(o.Provider,{value:n},e.children)}}}]);